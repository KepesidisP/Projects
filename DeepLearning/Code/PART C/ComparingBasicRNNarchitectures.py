# -*- coding: utf-8 -*-
"""ComparingBasicRNNArchitectures.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1udu3iV2iM9Ijln_WCf-Rtua1OcEDwKIJ
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, GRU, SimpleRNN
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, max_error
import matplotlib.pyplot as plt
from google.colab import drive
import os
import time

drive.mount('/content/drive')
os.chdir('/content/drive/MyDrive/DATA/WaterHeater')

X = np.loadtxt('Input_Data.txt', delimiter=',')
y = np.loadtxt('Output_Data.txt', delimiter=',')

#display three random plots
#each showing the total energy consumption and the appliance consumption for a specific time period.
for i in range(3):
    random_index = np.random.randint(X.shape[0])
    total_consumption = X[random_index]
    appliance_consumption = y[random_index]

    fig, ax1 = plt.subplots()

    ax1.set_xlabel('Time (minutes)')
    ax1.set_ylabel('Total Consumption (Watt)', color='tab:blue')
    ax1.plot(total_consumption, color='tab:blue')
    ax1.tick_params(axis='y', labelcolor='tab:blue')

    ax2 = ax1.twinx()
    ax2.set_ylabel('Appliance Consumption (Watt)', color='tab:red')
    ax2.plot(appliance_consumption, color='tab:red')
    ax2.tick_params(axis='y', labelcolor='tab:red')

    plt.title('Energy Consumption')
    plt.savefig('Energy Consumption'+str(i)+'.jpg', format='jpeg')
    plt.show()

X.shape

y.shape

norm_factor_input_data = np.loadtxt('WaterHeatermaxAgg.txt', delimiter=',')
norm_factor_input_data = norm_factor_input_data / 1000
#normalise data
norm_factor_output_data = np.loadtxt('WaterHeatermaxApp.txt', delimiter=',')
norm_factor_output_data = norm_factor_output_data / 1000

norm_factor_input_data

norm_factor_output_data

#normalise
normalized_input_values = X / norm_factor_input_data
normalized_output_values = y / norm_factor_output_data

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(normalized_input_values, normalized_output_values, test_size=0.2, random_state=42)

# Print the shapes of train and test sets
print("Train set shapes:", X_train.shape, y_train.shape)
print("Test set shapes:", X_test.shape, y_test.shape)

#callback for early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

seq_length = 120
input_dim = 1
output_dim = 1

# Define model architecture and compile
model_lstm = Sequential()
model_lstm.add(LSTM(64, input_shape=(seq_length, input_dim), return_sequences=True))
model_lstm.add(Dense(output_dim))
model_lstm.compile(optimizer=Adam(), loss=MeanSquaredError())

model_rnn = Sequential()
model_rnn.add(SimpleRNN(64, input_shape=(seq_length, input_dim), return_sequences=True))
model_rnn.add(Dense(output_dim))
model_rnn.compile(optimizer=Adam(), loss=MeanSquaredError())

model_gru = Sequential()
model_gru.add(GRU(64, input_shape=(seq_length, input_dim), return_sequences=True))
model_gru.add(Dense(output_dim))
model_gru.compile(optimizer=Adam(), loss=MeanSquaredError())

epochs = 30

# Train the models
history_lstm = model_lstm.fit(X_train, y_train, epochs=epochs, validation_split=0.2, callbacks=[early_stopping])
history_rnn = model_rnn.fit(X_train, y_train, epochs=epochs, validation_split=0.2, callbacks=[early_stopping])
history_gru = model_gru.fit(X_train, y_train, epochs=epochs, validation_split=0.2, callbacks=[early_stopping])

print("LSTM Training Losses:", history_lstm.history['loss'])
print("LSTM Validation Losses:", history_lstm.history['val_loss'])
print("RNN Training Losses:", history_rnn.history['loss'])
print("RNN Validation Losses:", history_rnn.history['val_loss'])
print("GRU Training Losses:", history_gru.history['loss'])
print("GRU Validation Losses:", history_gru.history['val_loss'])



train_predictions_lstm = model_lstm.predict(X_train)
test_predictions_lstm = model_lstm.predict(X_test)

train_predictions_rnn = model_rnn.predict(X_train)
test_predictions_rnn = model_rnn.predict(X_test)

train_predictions_gru = model_gru.predict(X_train)
test_predictions_gru = model_gru.predict(X_test)

# Denormalize the predictions
train_predictions_rnn_denorm = np.squeeze(train_predictions_rnn * norm_factor_output_data)
test_predictions_rnn_denorm = np.squeeze(test_predictions_rnn * norm_factor_output_data)

train_predictions_lstm_denorm = np.squeeze(train_predictions_lstm * norm_factor_output_data)
test_predictions_lstm_denorm = np.squeeze(test_predictions_lstm * norm_factor_output_data)

train_predictions_gru_denorm = np.squeeze(train_predictions_gru * norm_factor_output_data)
test_predictions_gru_denorm = np.squeeze(test_predictions_gru * norm_factor_output_data)

# Calculate RMSE, MAE, and max error for train and test sets
train_rmse_rnn = np.sqrt(mean_squared_error(y_train, train_predictions_rnn_denorm))
test_rmse_rnn = np.sqrt(mean_squared_error(y_test, test_predictions_rnn_denorm))

train_rmse_lstm = np.sqrt(mean_squared_error(y_train, train_predictions_lstm_denorm))
test_rmse_lstm = np.sqrt(mean_squared_error(y_test, test_predictions_lstm_denorm))

train_rmse_gru = np.sqrt(mean_squared_error(y_train, train_predictions_gru_denorm))
test_rmse_gru = np.sqrt(mean_squared_error(y_test, test_predictions_gru_denorm))

train_mae_rnn = mean_absolute_error(y_train, train_predictions_rnn_denorm)
test_mae_rnn = mean_absolute_error(y_test, test_predictions_rnn_denorm)

train_mae_lstm = mean_absolute_error(y_train, train_predictions_lstm_denorm)
test_mae_lstm = mean_absolute_error(y_test, test_predictions_lstm_denorm)

train_mae_gru = mean_absolute_error(y_train, train_predictions_gru_denorm)
test_mae_gru = mean_absolute_error(y_test, test_predictions_gru_denorm)

train_max_error_rnn = np.max(np.abs(y_train - train_predictions_rnn_denorm))
test_max_error_rnn = np.max(np.abs(y_test - test_predictions_rnn_denorm))

train_max_error_lstm = np.max(np.abs(y_train - train_predictions_lstm_denorm))
test_max_error_lstm = np.max(np.abs(y_test - test_predictions_lstm_denorm))

train_max_error_gru = np.max(np.abs(y_train - train_predictions_gru_denorm))
test_max_error_gru = np.max(np.abs(y_test - test_predictions_gru_denorm))

# Plot RMSE
plt.bar(['RNN', 'LSTM', 'GRU'], [test_rmse_rnn, test_rmse_lstm, test_rmse_gru])
plt.xlabel('Model')
plt.ylabel('RMSE')
plt.title('Test RMSE Comparison')
for i, value in enumerate([test_rmse_rnn, test_rmse_lstm, test_rmse_gru]):
    plt.text(i, value, str(round(value, 2)), ha='center', va='bottom')
plt.savefig('RMSE_comparison.jpg', format='jpeg')
plt.show()

# Plot MAE
plt.bar(['RNN', 'LSTM', 'GRU'], [test_mae_rnn, test_mae_lstm, test_mae_gru])
plt.xlabel('Model')
plt.ylabel('MAE')
plt.title('Test MAE Comparison')
for i, value in enumerate([test_mae_rnn, test_mae_lstm, test_mae_gru]):
    plt.text(i, value, str(round(value, 2)), ha='center', va='bottom')
plt.savefig('MAE_comparison.jpg', format='jpeg')
plt.show()

# Plot Max Error
plt.bar(['RNN', 'LSTM', 'GRU'], [test_max_error_rnn, test_max_error_lstm, test_max_error_gru])
plt.xlabel('Model')
plt.ylabel('Max Error')
plt.title('Test Max Error Comparison')
for i, value in enumerate([test_max_error_rnn, test_max_error_lstm, test_max_error_gru]):
    plt.text(i, value, str(round(value, 2)), ha='center', va='bottom')
plt.savefig('MaxError_comparison.jpg', format='jpeg')
plt.show()

# Select 2 sequences where the device was on
device_on_indices = np.where(y_test.sum(axis=1) > 0)[0][:2]
device_on_sequences = X_test[device_on_indices]
device_on_predictions_rnn = test_predictions_rnn[device_on_indices]
device_on_predictions_lstm = test_predictions_lstm[device_on_indices]
device_on_predictions_gru = test_predictions_gru[device_on_indices]

# Select 2 sequences where the device was off
device_off_indices = np.where(y_test.sum(axis=1) == 0)[0][:2]
device_off_sequences = X_test[device_off_indices]
device_off_predictions_rnn = test_predictions_rnn[device_off_indices]
device_off_predictions_lstm = test_predictions_lstm[device_off_indices]
device_off_predictions_gru = test_predictions_gru[device_off_indices]

# Plot device on sequences
for i in range(2):
    fig, ax1 = plt.subplots()
    ax1.set_xlabel('Time (minutes)')
    ax1.set_ylabel('Consumption (Watt)')
    ax1.plot(device_on_sequences[i], label='Real Data')
    ax1.plot(device_on_predictions_rnn[i], label='RNN Predictions')
    ax1.plot(device_on_predictions_lstm[i], label='LSTM Predictions')
    ax1.plot(device_on_predictions_gru[i], label='GRU Predictions')
    ax1.legend()
    plt.title('Device On - Sequence {}'.format(device_on_indices[i]))
    plt.savefig('Device_On_Sequence_{}.jpg'.format(device_on_indices[i]), format='jpeg')
    plt.show()

# Plot device off sequences
for i in range(2):
    fig, ax1 = plt.subplots()
    ax1.set_xlabel('Time (minutes)')
    ax1.set_ylabel('Consumption (Watt)')
    ax1.plot(device_off_sequences[i], label='Real Data')
    ax1.plot(device_off_predictions_rnn[i], label='RNN Predictions')
    ax1.plot(device_off_predictions_lstm[i], label='LSTM Predictions')
    ax1.plot(device_off_predictions_gru[i], label='GRU Predictions')
    ax1.legend()
    plt.title('Device Off - Sequence {}'.format(device_off_indices[i]))
    plt.savefig('Device_Off_Sequence_{}.jpg'.format(device_off_indices[i]), format='jpeg')
    plt.show()